{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bigscience/bloom-560m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "bloom = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\n",
    "bloom_tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n",
    "\n",
    "# bloom_tokenizer.add_bos_token = True  # This is not supported by the tokenizer, so we have to add it manually below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Add a bos token so we can compute the conditional likelihood of a sentence starting with nothing\n",
    "tokenizer.add_bos_token = True\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'सोमवार को, स्टैनफ़ोर्ड यूनिवर्सिटी स्कूल ऑफ़ मेडिसिन के वैज्ञानिकों ने एक नए डायग्नोस्टिक उपकरण के आविष्कार की घोषणा की जो कोशिकाओं को उनके प्रकार के आधार पर छाँट सकता है: एक छोटी प्रिंट करने योग्य चिप जिसे स्टैण्डर्ड इंकजेट प्रिंटर का उपयोग करके लगभग एक अमेरिकी सेंट के लिए निर्मित किया जा सकता है.'\n",
    "\n",
    "bos = bloom_tokenizer(bloom_tokenizer.bos_token, return_tensors='pt')\n",
    "encoded_input = bloom_tokenizer(input_text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     1, 242676,   1452,     15,  12145,   6313,  29190,    398,  15689,\n",
       "         152936,  36637, 122805,  51943,   1946,   1526,    858, 194876,   1992,\n",
       "           2122,  40161,  78762,  56124,    398,  10227,    952,  61503,    858,\n",
       "         216250,   1395,  29124,   1395,   4422,  36557, 121627,   1452,  15399,\n",
       "          11790,    858,  15338,   1633,   1796, 187418,  14932, 126031,   2122,\n",
       "          66293, 133986,   1236,   5672,  38806, 136518,  38409,  12145, 224394,\n",
       "          15689,   1720,  12912,    963,   3861, 133986,  12349,   1944,  22324,\n",
       "          29731,  34735,   2122,  44709, 109491,    858,   3698,  88813,   4730,\n",
       "          10356,  14932,    986,     17]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input[\"input_ids\"] = torch.concat([bos[\"input_ids\"], encoded_input[\"input_ids\"]], dim=1)\n",
    "encoded_input[\"attention_mask\"] = torch.concat([bos[\"attention_mask\"], encoded_input[\"attention_mask\"]], dim=1)\n",
    "\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for seq_len in range(1, encoded_input[\"input_ids\"].shape[1]):\n",
    "        _encoded_input = dict(\n",
    "            input_ids=encoded_input[\"input_ids\"][:, :seq_len],\n",
    "            attention_mask=encoded_input[\"attention_mask\"][:, :seq_len],\n",
    "        )\n",
    "\n",
    "        output = bloom(**_encoded_input)\n",
    "        log_prob = nn.Softmax(dim=-1)(output.logits)[0][0].log()\n",
    "\n",
    "        log_probs.append(log_prob[encoded_input[\"input_ids\"][0, seq_len]].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2273.030584335327"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-23.5910, -16.9164,  -5.5803,  ..., -16.2641, -16.2639, -16.2666],\n",
       "       grad_fn=<LogBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Softmax(dim=-1)(output.logits)[0][0].log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-lang-b0VyPkoq-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
